<?xml version="1.0" encoding="UTF-8"?>
<spiderman name="网易国内新闻采集">
    <property key="duration" value="60s" /><!-- 运行时间 0 表示永久，可以给 {n}s {n}m {n}h {n}d -->
    <property key="logger.level" value="INFO" /><!-- 日志级别 INFO DEBUG WARN ERROR OFF -->
    <property key="worker.download.size" value="10" /><!-- 下载线程数 -->
    <property key="worker.extract.size" value="10" /><!-- 页面抽取线程数 -->
    <property key="worker.result.size" value="10" /><!-- 结果处理线程数 -->
    <property key="worker.result.handler" value="net.kernal.spiderman.Bootstrap$ResultHandler" />
    <property key="queue.element.repeatable" value="false" /><!-- 队列元素是否允许重复，默认允许，若不允许，则使用重复检查器在元素入队列前进行检查 -->
    <property key="queue.checker.bdb.file" value="store/checker" /><!-- 检查器需要用到BDb存储 -->
    <seed url="http://news.163.com/domestic/" /><!-- 写死种子入口的方式 -->
    <extract><!-- 页面抽取规则 -->
        <extractor name="Text" class="net.kernal.spiderman.worker.extract.TextExtractor" isDefault="1" /><!-- 正文抽取器 -->
        <extractor name="Links" class="net.kernal.spiderman.worker.extract.LinksExtractor" /><!-- 链接抽取器 -->
        <page name="内容页" isUnique="1" extractor="Text"><!-- 如果不配置extractor属性，则使用默认的 -->
			<url-match-rule type="regex" value="^http://news\.163\.com/\d+/\d+/\d+/.*\.html#f\=dlist$" />
		</page>
		<page name="列表页" isUnique="1" extractor="Links">
			<url-match-rule type="regex">
			^http://news\.163\.com/((domestic/)|(special/0001124J/guoneinews_\d+\.html#headList))$
			</url-match-rule>
		</page>
	</extract>
</spiderman>
